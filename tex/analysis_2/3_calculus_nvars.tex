\chapter{Calcolo differenziale per funzioni in più variabili}\label{chap:nvars}

\section{Funzioni differenziabili}

\begin{definition}
    [Derivata parziale]
    Sia $f: A \subseteq \R^n \to \R$ con $A$ aperto. Si definisce derivata parziale $j$-esima di $f$ in $\vb{x_0}$ il valore del seguente limite, se esiste finito:
    $$
        \frac{\partial f}{\partial x_j} (\vb{x_0}) = \displaystyle \lim_{h\to 0} \frac{f(\vb{x_0} + h\vb{\hat{e}_j}) - f(\vb{x_0})}{h} \in \R
    $$
    dove $\vb{\hat{e}_j}$ è il j-esimo vettore della base canonica.
\end{definition}

\begin{definition}
    [Gradiente]
    Siano $A \subseteq \R^n$ un aperto, $\vb{x_0} \in A$ e $f:A \to \R$. Se esistono $\frac{\partial f}{\partial x_1}(\vb{x_0}),\dots,\\\frac{\partial f}{\partial x_n}(\vb{x_0})$, allora si definisce gradiente di $f$ in $\vb{x_0}$ il vettore $\bm{\nabla}f = \left( \frac{\partial f}{\partial x_1} (\vb{x_0}), \dots, \frac{\partial f}{\partial x_n}(\vb{x_0}) \right)$.
\end{definition}

\begin{definition}
    [Derivata direzionale]
    Siano $A \subseteq \R^n$ un aperto, $\vb{x_0} \in A$, $f:A \to \R$ e $\bm{\hat{\nu}} = (\nu_1,\dots,\nu_n) \in \R^n \tc \norm{\bm{\hat{\nu}}}=1$. Si definisce derivata direzionale di $f$ in $\vb{x_0}$ rispetto alla direzione $\bm{\hat{\nu}}$ il valore del seguente limite, se esiste finito:
    $$
        \frac{\partial f}{\partial \bm{\hat{\nu}}} (\vb{x_0}) = \displaystyle \lim_{h\to 0} \frac{f(\vb{x_0} + h\bm{\hat{\nu}}) - f(\vb{x_0})}{h} \in \R
    $$
\end{definition}

\begin{remark}
    La derivabilità in tutte le direzioni non assicura la continuità della funzione $f$ nel punto $\vb{x_0}$. Essa implica infatti solo che $f$ sia continua in $\vb{x_0}$ per ogni restrizione a rette passanti in $\vb{x_0}$, ma non in tutto un intorno di $\vb{x_0}$. È necessario un concetto più forte che viene definito di seguito.
\end{remark}

\begin{definition}
    [Funzione differenziabile]
    Siano $A \subseteq \R^n$ un aperto, $\vb{x_0} \in A$ e $f:A \to \R$. La funzione $f$ è differenziabile in $\vb{x_0}$ se esiste $\vb{m} = (m_1,\dots,m_n) \in \R^n \tc f(\vb{x}) = f(\vb{x_0}) + \ip{\vb{m}}{\vb{x} - \vb{x_0}} + o(\norm{\vb{x} - \vb{x_0}}) \with \norm{\vb{x}-\vb{x_0}}\to 0$.
\end{definition}

\begin{definition}
    [Operatore differenziale]
    Si definisce operatore differenziale e si indica con $\dd{f}_{\vb{x_0}} : \R^n \to \R$ l'operatore $\dd{f}_{\vb{x_0}}(\vb{h})=\ip{\vb{m}}{\vb{h}}$.
\end{definition}

\begin{theorem}
    [Proprietà delle funzioni differenziabili]\label{thm:prop_diff}
    Siano $A \subseteq \R^n$ un aperto, $\vb{x_0} \in A$ e $f:A \to \R$ una funzione differenziabile. Allora:
    \begin{enumerate}
        \item $f$ è continua in $\vb{x_0}$
        \item $f$ è derivabile parzialmente in $\vb{x_0}$ e $\vb{m} = \bm{\nabla}f(\vb{x_0})$
        \item $f$ è derivabile in qualunque direzione $\bm{\hat{\nu}} \tc \norm{\bm{\hat{\nu}}}=1$ e $\frac{\partial f}{\partial \bm{\hat{\nu}}} = \ip{\grad f(\vb{x_0})}{\bm{\hat{\nu}}}$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Si dimostra ciascun punto separatamente.
    \begin{enumerate}
        \item Si verifica che la distanza fra $f(\vb{x})$ e $f(\vb{x_0})$ tende a $0$ se $\vb{x}$ tende a $\vb{x_0}$.
        \begin{align*}
            \abs{f(\vb{x})-f(\vb{x_0})}&=\abs{\ip{\vb{m}}{\vb{x}-\vb{x_0}}+o(\norm{\vb{\vb{x}-\vb{x_0}}})}\leq\\
            &\leq\abs{\ip{\vb{m}}{\vb{\vb{x}-\vb{x_0}}}}+o(\norm{\vb{\vb{x}-\vb{x_0}}})\leq\\
            &\leq\norm{\vb{m}}\norm{\vb{x}-\vb{x_0}}+o(\norm{\vb{x}-\vb{x_0}})\xrightarrow{\vb{x}\to\vb{x_0}} 0
        \end{align*}

        \item Si studia il limite che definisce la derivata parziale $j$-esima di $f$ in $\vb{x_0}$.
        \begin{align*}
            \frac{\partial f}{\partial x_j}&=\lim_{t\to 0}\frac{f(\vb{x_0}+t\hat{\vb{e}}_j)-f(\vb{x_0})}{t}=\\
            &=\lim_{t\to 0}\frac{\ip{\vb{m}}{t\hat{\vb{e}}_j} + o(\abs{t})}{t}=\\
            &=\ip{\vb{m}}{\hat{\vb{e}}_j}=m_j
        \end{align*}
        La derivata parziale di $f$ rispetto a $x_j$ è la $j$-esima componente del vettore $\vb{m}$, quindi $\vb{m}=\grad f(\vb{x_0})$.

        \item Come nel punto \textit{2}, si studia il limite che definisce la derivata direzionale di $f$ in $\vb{x_0}$.
        \begin{align*}
            \frac{\partial f}{\partial \hat{\bm \nu}}&=\lim_{t\to 0}\frac{f(\vb{x_0}+t\hat{\bm \nu})-f(\vb{x_0})}{t}=\\
            &=\lim_{t\to 0}\frac{\ip{\grad f(\vb{x_0})}{t\hat{\bm \nu}} + o(\abs{t})}{t}=\\
            &=\ip{\grad f(\vb{x_0})}{\hat{\bm \nu}}
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{remark}
    Si noti che dal punto \textit{3} del teorema \ref{thm:prop_diff} si evince che la direzione di massima variazione della funzione nel punto $\vb{x_0}$ sia esattamente la direzione del gradiente di $f$ in $\vb{x_0}$.
\end{remark}

\begin{definition}
    [Insieme di livello]
    Siano $A \subseteq \R^n$ un aperto, $f:A \to \R$ e $c \in \R$. Si definisce insieme di livello l'insieme $M_c = \{ \vb{x} \in A : f(\vb{x})=c\}\subseteq A$.
\end{definition}

\begin{theorem}
    [del differenziale totale]\label{thm:diff_tot}
    Siano $A \subseteq \R^n$ un aperto e $f: A \to \R$ derivabile in $A$. Se $\frac{\partial f}{\partial x_j} (\vb{x})$ sono continue in $\vb{x_0} \ \forall j \in [n]$, allora $f$ è differenziabile in $\vb{x_0}$.
\end{theorem}

\begin{proof}[Dimostrazione per n=2.]
    Per la dimostrazione, si scrive $f(x,y)-f(x_0,y_0)=f(x,y)+f(x,y_0)-f(x,y_0)-f(x_0,y_0)$. Applicando il teorema del valor medio di Lagrange al primo e al terzo termine, si ottiene che
    \begin{equation}\label{eq:diff_tot_1}
        f(x,y)-f(x,y_0)=\frac{\partial f}{\partial y}(x,c)[y-y_0]=\left[\frac{\partial f}{\partial y}(x_0,y_0)+o(1)\right](y-y_0) \with (x,y)\to (x_0,y_0)
    \end{equation}
    Applicando lo stesso ragionamento al secondo e al quarto termine, si ottiene che:
    \begin{equation}\label{eq:diff_tot_2}
        f(x,y_0)-f(x_0,y_0)=\frac{\partial f}{\partial x}(d,y_0)[x-x_0]=\left[\frac{\partial f}{\partial x}(x_0,y_0)+o(1)\right](x-x_0) \with (x,y)\to(x_0,y_0)
    \end{equation}
    Unendo le equazioni \eqref{eq:diff_tot_1} e \eqref{eq:diff_tot_2}, si può scrivere il seguente limite:
    \begin{align*}
        &\lim_{\norm{(x-x_0,y-y_0)}\to 0}\frac{f(x,y)-f(x_0,y_0)-\ip{\grad f(x_0,y_0)}{(x-x_0,y-y_0)}}{\norm{(x-x_0,y-y_0)}}=\\
        =&\lim_{\norm{(x-x_0,y-y_0)}\to 0}o(1)\frac{x-x_0+y-y_0}{\norm{(x-x_0,y-y_0)}}=0
    \end{align*}
    Che è esattamente la definizione di differenziabilità, da cui la tesi. Si noti che la frazione nell'ultimo passaggio è limitata per $(x,y)\to(x_0,y_0)$.
\end{proof}

\begin{theorem}
    [Funzioni a gradiente nullo]
    Sia $A \subseteq \R^n$ aperto e connesso. Se $f: A \to \R$ è derivabile in $A$ e $\grad f (\vb{x})=(0,\dots,0) \ \forall \vb{x} \in A$, allora $f$ è costante.
\end{theorem}

\begin{proof}
    $f$ ha le derivate parziali continue in tutto $A$, quindi è differenziabile per il teorema \ref{thm:diff_tot}. Ai fini di questo passaggio, si consideri $A\subseteq\R^2$ e sia $(x_0,y_0)\in\R^2$. Sia $(x,y)$ un punto qualsiasi nella palla di raggio $\varepsilon>0$ centrata in $(x_0,y_0)$. Applicando il teorema del valor medio di Lagrange, si ottiene che
    \begin{align*}
        f(x,y)-f(x_0,y_0)&=f(x,y)+f(x,y_0)-f(x,y_0)-f(x_0,y_0)=\\
        &=\frac{\partial f}{\partial x}(c,y_0)[x-x_0]+\frac{\partial f}{\partial y}(x,d)[y-y_0]=0
    \end{align*}
    Quindi $\exists \varepsilon > 0 \tc \forall \vb{x}\in B_\varepsilon(\vb{x_0})\cap A\ f(\vb{x})=f(\vb{x_0})$.
    
    $A$ è un insieme aperto, quindi essendo connesso è anche connesso per archi. Questo significa che, per qualsiasi $\vb{x},\vb{y} \in A$ è possibile costruire un'applicazione $\vb{r}:[0,1]\to A$ continua tale che $\vb{r}(0)=\vb{x} \e \vb{r}(1)=\vb{y}$. Sia $\overline{t}=\sup\{t \in [a,b]:f(\vb{r}(t))=f(\vb{x})\}$. Per assurdo, si supponga che sia $\overline{t}<1$. Questo è in diretta contraddizione con quanto detto in precedenza, perché vorrebbe dire che non è possibile trovare una palla centrata in $\vb{r}(\overline{t})$ in cui la funzione è costante. Quindi $\overline{t}=1$ e la funzione è costante.
\end{proof}

\section{Derivate di ordine superiore}

\begin{definition}
    [Derivata di ordine $k$]
    Siano $A \subseteq \R^n$ un aperto, $\vb{x_0} \in A$ e $f:A \to \R$. Se $\displaystyle \exists \frac{\partial f}{\partial x_{i_1}}, \frac{\partial^2 f}{\partial x_{i_1} \partial x_{i_2}},\dots,\frac{\partial ^{k-1}f}{\partial x_{i_1} \cdots \partial x_{i_{k-1}}} : B_\varepsilon(\vb{x_0}) \to \R$, si definisce derivata $k$-esima di $f$ in $\vb{x_0}$ rispetto a $x_{i_1},\dots,x_{i_{k}}$ il valore del seguente limite se esiste finito:
    $$
        \lim_{h\to 0}\frac{\frac{\partial^{k-1}f}{\partial x_{i_1}\cdots \partial x_{i_{k-1}}}(\vb{x_0}+ h \vb{\hat{e}_{i_k}})- \frac{\partial^{k-1}f}{\partial x_{i_1}\cdots \partial x_{i_{k-1}}}(\vb{x_0})}{h} \in \R
    $$
\end{definition}

\begin{definition}
    [Insieme $\C{k}(A, \R)$]
    Si definisce l'insieme delle funzioni derivabili con continuità $k$ volte in $A\subseteq\R^n$ aperto:
    $$
        \displaystyle \C{k}(A,\R) = \left\{ f: A \to \R \tc \ \exists \frac{\partial f}{\partial x_i},\dots,\frac{\partial ^2 f}{\partial x_i \partial x_j}, \dots, \frac{\partial ^k f}{\partial x_{i_1}\cdots\partial x_{i_k}} \text{ continue in } A \right\}
    $$
\end{definition}

\begin{remark}
    Si noti che per il teorema \ref{thm:diff_tot} è sufficiente che siano continue le derivate $k$-esime, perché le altre lo sono di conseguenza. Devono comunque essere continue tutte le combinazioni possibili di derivate, che crescono molto velocemente ($n^k$ combinazioni per le derivate di ordine $k$).
\end{remark}

\begin{theorem}
    [di Schwarz]\label{thm:schwarz}
    Siano $A \subseteq \R^n$ un aperto e $f: A \to \R$ derivabile due volte in $A$. Se $\frac{\partial ^2 f}{\partial x_i \partial x_j}$ e $\frac{\partial^2 f}{\partial x_j \partial x_i}$ sono continue in $\vb{x_0} \in A$, allora
    $$
        \frac{\partial ^2 f}{\partial x_i \partial x_j}(\vb{x_0}) = \frac{\partial^2  f}{\partial x_j \partial x_i}(\vb{x_0})
    $$
\end{theorem}

\begin{proof}
    [Dimostrazione per n=2.]
    Sia $F(x)=f(x,y)-f(x,y_0)$. Applicando due volte il teorema del valor medio di Lagrange si ottiene che
    \begin{align*}
        F(x)-F(x_0)&=F'(c)[x-x_0]=\left(\frac{\partial f}{\partial x}(c,y)-\frac{\partial f}{\partial x}(c,y_0) \right)[x-x_0]=\\
        &=\frac{\partial^2 f}{\partial x \partial y}(c,d)[x-x_0][y-y_0]
    \end{align*}
    Inoltre,
    $$
        F(x)-F(x_0)=f(x,y)-f(x,y_0)-f(x_0,y)+f(x_0,y_0)=G(y)-G(y_0)
    $$
    dove $G(y)=f(x,y)-f(x_0,y)$. Applicando nuovamente Lagrange come in precedenza, si ottiene
    \begin{align*}
        G(y)-G(y_0)&=G'(\xi)[y-y_0]=\left(\frac{\partial f}{\partial y}(x,\eta)-\frac{\partial f}{\partial y}(x_0,\eta)\right)[y-y_0]=\\
        &=\frac{\partial ^2f}{\partial y\partial x}(\xi,\eta)[x-x_0][y-x_0]
    \end{align*}
    Essendo le derivate seconde continue, si possono mandare $(c,d)\to(x_0,y_0) \e (\xi,\eta)\to(x_0,y_0)$ e ottenere la tesi:
    $$
        \frac{\partial^2 f}{\partial x \partial y}(x_0,y_0)=\frac{\partial^2 f}{\partial y \partial x}(x_0,y_0)
    $$
\end{proof}

\begin{definition}
    [Matrice hessiana]
    Siano $A \subseteq \R^n$ un aperto e $f: A \to \R$ derivabile due volte in $A$. Si definisce matrice hessiana di $f$ in $\vb{x_0}$ la matrice
    $$
        H_f(\vb{x_0}) =
        \begin{bmatrix}
            \grad \frac{\partial f}{\partial x_1}(\vb{x_0})\\
            \vdots\\
            \grad \frac{\partial f}{\partial x_n}(\vb{x_0})
        \end{bmatrix}
    $$
\end{definition}

\begin{remark}
    Per il teorema \ref{thm:schwarz}, se $f$ è $\C{2}$ in $\vb{x_0} \in A$, allora $H_f(\vb{x_0})$ è simmetrica.
\end{remark}

\begin{theorem}
    [Formula di Taylor di ordine $k$]
    Sia $A \subseteq \R^n$ un aperto. Se $f \in \C{k}(A, \R)$, allora vale la seguente
    \begin{align*}
        f(\vb{x_0}+ \vb{h})= f(\vb{x_0})&+\sum_{j=1}^{n}\frac{\partial f}{\partial x_j}(\vb{x_0})h_j + \frac{1}{2}\sum_{i,j=1}^{n}\frac{\partial^2 f}{\partial x_i \partial x_j}(\vb{x_0})h_ih_j+\cdots+\\
        &+\frac{1}{k!}\sum_{i_1,\dots,i_k=1}^n\frac{\partial^k f}{\partial x_{i_1}\cdots\partial x_{i_k}}(\vb{x_0})h_{i_1}\cdots h_{i_k} +\\
        &+o (\norm{\vb{h}}^k) \with \norm{\vb{h}} \to 0
    \end{align*}
    \qed
\end{theorem}

\begin{remark}
    In particolare, per $n=2$ vale
    $$
        f(\vb{x_0} + \vb{h}) = f(\vb{x_0})+\ip{\grad f(\vb{x_0})}{\vb{h}} + \frac{1}{2}\ip{\vb{h}}{H_f(\vb{x_0})\vb{h}}+o(\norm{\vb{h}}^2) \with \norm{\vb{h}} \to 0
    $$
\end{remark}

\section{Punti critici liberi}

\begin{definition}
    [Massimo (minimo) locale]\label{def:minmax}
    Sia $A \subseteq \R^n$ e $f: A \to \R$. Il punto $\vb{x_0}$ è un punto di massimo (minimo) locale per $f$ se $\exists \delta > 0 \tc f(\vb{x}) \leq f(\vb{x_0})\ (f(\vb{x})\geq f(\vb{x_0})) \ \forall \vb{x} \in A \cap B_\delta (\vb{x_0})$.
\end{definition}

\begin{definition}
    [Punto di sella]\label{def:sella}
    Sia $A \subseteq \R^n$ e $f: A \to \R$. Il punto $\vb{x_0}$ è un punto di sella per $f$ se $\grad f(\vb{x_0})=\vb{0}$ e $\forall \varepsilon > 0 \ \exists \vb{x_1}, \vb{x_2} \in A \cap B_\varepsilon(\vb{x_0}) \tc f(\vb{x_1}) > f(\vb{x_0}) > f(\vb{x_2})$.
\end{definition}

\begin{remark}
    Si noti che nelle definizioni \ref{def:minmax} e \ref{def:sella} non si richiede alcuna regolarità di $f$ e alcuna proprietà particolare di $A$.
\end{remark}

\begin{definition}
    [Punto critico (o stazionario)]
    Siano $A \subseteq \R^n$ un aperto e $f: A\to \R$. $\vb{x_0} \in A$ è un punto critico (o stazionario) di $f$ se $\exists \grad f(\vb{x_0})$ e $\grad f(\vb{x_0})=\vb{0}$.
\end{definition}

\begin{theorem}
    [di Fermat]\label{thm:fermat}
    Sia $A \subseteq \R^n$ un aperto e $f: A \to \R$. Se $\vb{x_0} \in A$ è massimo o minimo locale per $f$ ed esiste il gradiente di $f$ in $\vb{x_0}$, allora $\grad f(\vb{x_0})=\vb{0}$, cioè $\vb{x_0}$ è punto critico di $f$.
\end{theorem}

\begin{proof}
    Sia $F_j(t)=f(\vb{x_0}+t\hat{\vb{e}}_j):(-\varepsilon,\varepsilon)\to\R$, dove $\hat{\vb{e}}_j$ è il $j$-esimo vettore della base canonica. Per costruzione $t=0$ è un punto di massimo o minimo locale per $F_j$, di conseguenza per il teorema di Fermat in una dimensione $$\frac{\dd F_j}{\dd t}(0)=0$$
    Inoltre, si ha che
    \begin{align*}
        \frac{\dd F_j}{\dd t}(0) =\lim_{h\to0}\frac{F(h)-F(0)}{h}= \lim_{h\to 0}\frac{f(\vb{x_0}+h\hat{\vb{e}}_j)-f(\vb{x_0})}{h}=\frac{\partial f}{\partial x_j}(\vb{x_0})
    \end{align*}
    Dove nell'ultima uguaglianza si è usato il fatto che per ipotesi esistono tutte le derivate parziali di $f$. In conclusione, ogni componente del gradiente è nulla quindi si ha la tesi.
\end{proof}

\subsection{Forme quadratiche}

Sia $M \in \mathcal{M}_{n \times n}(\R)$ una matrice quadrata di ordine $n$. La seguente espressione si chiama forma quadratica associata alla matrice $M$:
$$
    q_M(\vb{h}) = \ip{\vb{h}}{M\vb{h}} \in \R
$$
La forma quadratica può essere:
\begin{itemize}
    \item Definita positiva se $\forall \vb{h}\neq \vb{0}, \ q_M(\vb{h})> 0$
    \item Definita negativa se $\forall \vb{h}\neq \vb{0}, \ q_M(\vb{h}) < 0$
    \item Semidefinita positiva se $\forall \vb{h}\in \R^n, \ q_M(\vb{h}) \geq 0$ e $\exists \vb{k} \tc q_M(\vb{k})=0$
    \item Semidefinita negativa se $\forall \vb{h}\in \R^n, \ q_M(\vb{h}) \leq 0$ e $\exists \vb{k} \tc q_M(\vb{k})=0$
    \item Indefinita se $\exists \vb{h}, \vb{k} \in \R^n \tc q_M(\vb{h}) < 0 < q_M(\vb{k})$
\end{itemize}

Matrici diverse possono essere associate alla stessa forma quadratica, infatti è sufficiente che abbiano gli stessi elementi sulla diagonale e che la somma $a_{ij}+a_{ji}=b_{ij}+b_{ji} \ \forall i\neq j\with i,j \in [n]$. Fra tutte le matrici associate alla stessa forma quadratica, ci si può restringere alle matrici simmetriche (che sono sempre diagonalizzabili!), per cui esistono criteri di classificazione efficaci.

\begin{theorem}
    [Classificazione delle forme quadratiche e segno degli autovalori]
    Sia $A \in \mathcal{M}_{n\times n}(\R)$ tale che $A=A^T$ e $q_A$ la forma quadratica associata ad $A$. Sia la coppia $(p,q)$ la segnatura della matrice $A$, dove $p$ rappresenta la somma delle molteplicità algebriche degli autovalori positivi e $q$ la somma delle molteplicità algebriche degli autovalori negativi. Allora:
    \begin{enumerate}
        \item $q_A$ è definita positiva $\iff (p,q)=(n,0)$
        \item $q_A$ è definita negativa $\iff (p,q)=(0,n)$
        \item $q_A$ è semidefinita positiva $\iff (p,q)=(m,0) \with 0< m < n$
        \item $q_A$ è semidefinita negativa $\iff (p,q)=(0,l) \with 0< l < n$
        \item $q_A$ è indefinita $\iff (p,q)=(m,l) \with 0<m<n,0<l<n$
        \qed
    \end{enumerate}
\end{theorem}

\begin{theorem}
    [Criterio di Sylvester]
    Sia $A \in \mathcal{M}_{n\times n}(\R) \tc A=A^T\neq \vb{O}$ e sia $A_k$ il minore principale nord-ovest di ordine $k$ ottenuto selezionando le prime $k$ righe e le prime $k$ colonne di $A$. Allora la matrice $A$ è:
    \begin{enumerate}
        \item Definita positiva $\iff \det A_k > 0 \ \forall k \in [n]$
        \item Definita negativa $\iff \det A_k < 0$ se $k$ è dispari e $\det A_k > 0$ se $k$ è pari $ \ \forall k \in [n]$
        \item Semidefinita positiva $\iff \det A_k \geq 0 \ \forall k \in [n]$ e $\exists j \in [n] \tc \det A_j = 0$
        \item Semidefinita negativa $\iff \det A_k \leq 0$ se $k$ è dispari e $\det A_k \geq 0$ se $k$ è pari $ \ \forall k \in [n]$ e $\exists j \in [n] \tc \det A_j=0$
        \item Indefinita altrimenti
        \qed
    \end{enumerate}
\end{theorem}

\subsection{Classificazione dei punti critici}

\begin{theorem}
    [Classificazione con la matrice hessiana]
    Sia $f \in \C{2}(A, \R) \with A\subseteq \R^n$ aperto. Allora:
    \begin{enumerate}
        \item Se $\vb{x_0} \in A$ è punto di minimo locale per $f$, allora $\grad f(\vb{x_0})=\vb{0}$ e $H_f(\vb{x_0})$ è o definita positiva o semidefinita positiva
        \item Se $\vb{x_0} \in A$ è punto di massimo locale per $f$, allora $\grad f(\vb{x_0})=\vb{0}$ e $H_f(\vb{x_0})$ è o definita negativa o semidefinita negativa
        \item Se $\grad f(\vb{x_0})=\vb{0}$ e $H_f(\vb{x_0})$ è definita positiva, allora $\vb{x_0}$ è punto di minimo locale per $f$
        \item Se $\grad f(\vb{x_0})=\vb{0}$ e $H_f(\vb{x_0})$ è definita negativa, allora $\vb{x_0}$ è punto di massimo locale per $f$
        \item Se $\grad f(\vb{x_0})=\vb{0}$ e $H_f(\vb{x_0})$ è indefinita, allora $\vb{x_0}$ è un punto di sella per $f$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Si dimostrano i punti \textit{1} e \textit{3}. I punti \textit{2} e \textit{4} hanno la dimostrazione identica, il punto \textit{5} non è stato affrontato.
    \begin{enumerate}
        \item $\vb{x_0}$ è un punto di minimo locale per $f$ e il suo gradiente esiste perché $f\in\C{2}$, allora per il teorema \ref{thm:fermat} $\grad f(\vb{x_0}) = \vb{0}$. Sia $F:(-\varepsilon,\varepsilon)\to \R$ definita come segue:
        $$
            F(t)=f(\vb{x_0}+t\hat{\bm\nu})\with \hat{\bm\nu} \in \R^n\tc \norm{\hat{\bm\nu}}=1
        $$
        Allora per costruzione $t=0$ è un punto di minimo locale per $F$. Per questo motivo,
        \begin{align*}
            0&\leq F''(0)=\lim_{h\to 0}\frac{\frac{\partial f}{\partial \hat{\bm\nu}}(\vb{x_0}+h\hat{\bm\nu})-\frac{\partial f}{\partial \hat{\bm\nu}}(\vb{x_0})}{h}=\\
            &=\lim_{h\to 0}\sum_{i=1}^n\frac{[\partial_i f(\vb{x_0 + h\hat{\bm\nu}})-\partial_i f(\vb{x_0})]\nu_i}{h}=\\
            &=\sum_{i=1}^{n}\frac{\partial}{\partial \hat{\bm\nu}}\frac{\partial f}{\partial x_i}(\vb{x_0})\nu_i=\\
            &=\sum_{i=1}^n\sum_{j=1}^{n}=\frac{\partial^2 f}{\partial x_i \partial x_j}(\vb{x_0})\nu_i\nu_j=\ip{\hat{\bm\nu}}{H_f(\vb{x_0})\hat{\bm\nu}}
        \end{align*}
        Questo significa che $H_f(\vb{x_0})$ è definita positiva o semidefinita positiva.
        \addtocounter{enumi}{1}
        \item Si applichi la formula di Taylor alla funzione $f$ nel punto $\vb{x_0}$. Allora, essendo $\grad f(\vb{x_0})=\vb{0}$,
        $$
            f(\vb{x_0}+\vb{h})-f(\vb{x_0}) = \frac{1}{2}\ip{\vb{h}}{H_f(\vb{x_0})\vb{h}}+o(\norm{h}^2) \with \norm{h}\to 0
        $$
        Sia $\overline{\lambda}>0$ il minimo degli autovalori di $H_f(\vb{x_0})$, allora vale la seguente maggiorazione:
        \begin{gather*} 
            \ip{\vb{h}}{H_f(\vb{x_0})\vb{h}}\geq \overline{\lambda}\norm{\vb{h}^2}\\
            \then f(\vb{x_0}+\vb{h})-f(\vb{x_0})\geq \norm{h}^2\left(\frac{1}{2}\overline{\lambda}+o(1)\right)\geq 0
        \end{gather*}
        Per cui $\vb{x_0}$ è un punto di minimo locale per $f$.
    \end{enumerate}
\end{proof}