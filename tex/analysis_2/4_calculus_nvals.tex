\chapter{Calcolo differenziale per funzioni a valori vettoriali}

\section{Funzioni derivabili e differenziabili}

\begin{definition}
    [Funzione derivabile]
    Sia $\vecf=(f_1,\dots,f_k): A\subseteq \R^n \to \R^k \with A$ aperto. $\vecf$ è derivabile in $\vb{x_0} \in A$ se $\exists \frac{\partial f_1}{\partial x_i}(\vb{x_0}),\dots,\frac{\partial f_k}{\partial x_i}(\vb{x_0}) \in \R \ \forall i \in [n]$. In questo caso si definisce la matrice jacobiana di $\vecf$ in $\vb{x_0}$ come segue:
    $$
        J_{\vecf}(\vb{x_0})=
        \begin{bmatrix}
            \grad f_1 (\vb{x_0})\\
            \vdots \\
            \grad f_k (\vb{x_0})
        \end{bmatrix}
        \in \mathcal{M}_{k\times n}(\R)
    $$
\end{definition}

\begin{definition}
    [Funzione differenziabile]
    Sia $\vecf=(f_1,\dots,f_k): A\subseteq \R^n \to \R^k \with A$ aperto. $\vecf$ è differenziabile in $\vb{x_0} \in A$ se $\exists M \in \mathcal{M}_{k\times n}(\R)$ tale che
    $$
        \vecf(\vb{x_0} + \vb{h})= \vecf(\vb{x_0}) + M\vb{h} + \bm o (\norm{\vb{h}}) \in \R^k \with \norm{\vb{h}}\to 0
    $$
\end{definition}

\begin{theorem}
    [Condizioni per differenziabilità]
    Sia $A \subseteq \R^n$ un aperto e $\vecf= (f_1,\dots,f_k):A \to \R^k$.
    \begin{enumerate}
        \item $\vecf$ è differenziabile in $\vb{x_0} \in A \iff f_1,\dots,f_k$ sono differenziabili in $\vb{x_0} \in A$. In tal caso
        $$
            M=J_{\vecf}(\vb{x_0})=
            \begin{bmatrix}
                \grad f_1(\vb{x_0})\\
                \vdots\\
                \grad f_k(\vb{x_0})
            \end{bmatrix}
            \in \mathcal{M}_{k\times n}(\R)
        $$
        \item Se $\vecf$ è differenziabile in $\vb{x_0} \in A$, allora $\vecf$ è continua in $\vb{x_0}$
        \item Se $\vecf$ è derivabile in $\vb{x_0} \in A$ e le derivate parziali sono tutte continue in $\vb{x_0}$, allora $\vecf$ è differenziabile
        \qed
    \end{enumerate}
\end{theorem}

\begin{theorem}
    [di differenziabilità della funzione composta]
    Siano $A\subseteq \R^n, B \subseteq \R^k$ aperti. Siano $\vecf: A \to B$ differenziabile in $\vb{x_0} \in A$ e $\bm g:B \to \R^l$ differenziabile in $\vb{y_0} = \vecf(\vb{x_0})$, allora $\bm g \circ \vecf : A \to \R^l$ è differenziabile in $\vb{x_0}$ e $J_{\bm g \circ \vecf}(\vb{x_0})=J_{\bm g}(\vecf (\vb{x_0}))\cdot J_{\vecf}(\vb{x_0}) \in \mathcal{M}_{l\times n}$.
    \qed
\end{theorem}

\begin{definition}
    [Diffeomorfismo]
    Siano $A,B \subseteq \R^n$ aperti. La funzione $\vecf: A \to B$ è un diffeomorfismo $\C{k}$ se $\vecf$ è iniettiva e suriettiva e $\vecf, \vecf^{-1}$ sono $\C{k}$ nei rispettivi domini.
\end{definition}

\begin{theorem}
    [Differenziale della funzione inversa]
    Siano $A,B \subseteq \R^n$ aperti e sia $\vecf:A\to B$ un diffeomorfismo $\C{1}$. Allora vale che
    $$
        J_{\vecf^{-1}}(\vb{y}) = \left(J_{\vecf}\left(\vecf^{-1}\left(\vb{y}\right)\right) \right)^{-1} \ \forall \vb{y} \in B
    $$
\end{theorem}

\begin{proof}
    $\vecf$ è invertibile perché è un diffeomorfismo. Di conseguenza,
    \begin{gather*}
        \vecf\left(\vecf^{-1}(\vb{y})\right)=\vb{y}
        \then J_{\vecf}\left(\vecf^{-1}(\vb{y})\right)\cdot J_{\vecf^{-1}}(\vb{y})=\text{id}_{n\times n}
        \then J_{\vecf^{-1}}(\vb{y})=\left(J_{\vecf}\left(\vecf^{-1}(\vb{y})\right)\right)^{-1}
    \end{gather*}
\end{proof}

\section{Varietà regolari}

\begin{definition}
    [Varietà regolare $(n-k)$-dimensionale]
    Siano $A \subseteq \R^n$, $\bm g=(g_1,\dots,g_k):A\to\R^k \in \C{1}$. Detto $\Gamma =\{\vb{x}\in A : g_1(\vb{x})=0,\dots,g_k(\vb{x})=0\}$ l'insieme delle soluzioni del sistema di equazioni, $\Gamma$ è una varietà regolare $(n-k)$-dimensionale se $\forall \vb{x} \in \Gamma \ J_{\bm g}(\vb{x})$ ha rango uguale a $k$.
\end{definition}

\begin{definition}
    [Vettore tangente]
    Il vettore $\vb{h} \in \R^n$ è tangente a $\Gamma$ in $\vb{x_0}$ se $\exists \bm\varphi: (-\varepsilon, \varepsilon) \to \Gamma \subseteq \R^n$ di classe $\C{1} \tc \bm\varphi (0) = \vb{x_0} \e \bm\varphi ' (0) = \vb{h}$.
\end{definition}

Per ricavare i vettori appartenenti allo spazio tangente si definisce $\vb{G}=(\vb{g}\circ\bm\varphi):(-\varepsilon,\varepsilon)\to\R^k$. Considerando che la funzione $\vb{g}$ non varia spostandosi sulla varietà regolare (cioè variando il parametro $t$ della funzione $\bm\varphi$), si ha che:
$$
    \vb{0}=\frac{\dd \vb{G}}{\dd t}(0)=J_{\vb{g}}(\bm\varphi(0))\cdot \bm\varphi'(0)=
    \begin{bmatrix}
        \ip{\grad g_1(\vb{x_0})}{\vb{h}}\\
        \vdots\\
        \ip{\grad g_k(\vb{x_0})}{\vb{h}}
    \end{bmatrix}
$$
Dall'ultimo termine si deduce che $\vb{h}$ deve essere ortogonale a tutti i gradienti delle componenti della funzione $\vb{g}$.

\begin{definition}
    [Spazio tangente]
    Se $\Gamma$ è una varietà regolare $(n-k)$-dimensionale e $\vb{x_0} \in \Gamma$, si definisce lo spazio tangente come segue:
    $$
        T_{\vb{x_0}}\Gamma = \{ \vb{h}=(h_1,\dots,h_n) \in \R^n : \ip{\grad g_1(\vb{x_0})}{\vb{h}}=0,\dots,\ip{\grad g_k(\vb{x_0})}{\vb{h}}=0 \}
    $$
\end{definition}

\begin{remark}
    Si noti che $\grad g_1(\vb{x_0}),\dots,\grad g_k (\vb{x_0})$ costituisce un sistema di vettori linearmente indipendenti perché $\rank(J_{\bm g})=k \ \forall \vb{x} \in \Gamma$. Per questo motivo, $\dim T_{\vb{x_0}}\Gamma= n-k$.
\end{remark}

\begin{definition}
    [Spazio normale]
    Se $\Gamma$ è una varietà regolare $(n-k)$-dimensionale e $\vb{x_0} \in \Gamma$, si definisce lo spazio normale come segue:
    $$
        N_{\vb{x_0}}\Gamma = \left\{ \vb{h}=(h_1,\dots,h_n) \in \R^n : \exists \lambda_1, \dots, \lambda_k \in \R \e \vb{h}=\sum_{j=1}^k \lambda_j \grad g_j(\vb{x_0}) \right\}
    $$
    Si noti che $\dim N_{\vb{x_0}}\Gamma = k$.
\end{definition}

\begin{definition}
    [Iperpiano tangente]
    A partire dalla nozione di spazio tangente si può definire l'iperpiano tangente nel punto $\vb{x_0}$, che è costituito dall'insieme dei punti $\vb{x}$ tali che il loro vettore distanza dal punto $\vb{x_0}$ appartenga allo spazio tangente a $\Gamma$:
    $$
        I= \{ \vb{x} \in \R^n :  \ip{\grad g_1(\vb{x_0})}{\vb{x}-\vb{x_0}}=0,\dots,\ip{\grad g_k(\vb{x_0})}{\vb{x}-\vb{x_0}}=0 \}
    $$
\end{definition}

\section{Teorema di Dini}

\begin{theorem}
    [Caso con $1$ equazione in $2$ incognite]
    Sia $A \subseteq \R^2$ aperto, $g \in \C{1}(A, \R)$ e $(x_0,y_0) \in A \tc g(x_0, y_0)=0 \e \partial_y g(x_0,y_0)\neq 0$.
    Allora,
    \begin{enumerate}
        \item $\exists \delta, \varepsilon>0 \tc W=[x_0-\delta,x_0+\delta]\times[y_0-\varepsilon,y_0+\varepsilon] \subseteq A$
        \item $\exists f \in \C{1}([x_0-\delta,x_0+\delta],[y_0-\varepsilon,y_0+\varepsilon]) \tc g(x,y)=0 \with (x,y) \in W \iff y=f(x) \with x \in [x_0-\delta,x_0+\delta]$
    \end{enumerate}
    In altre parole, se si restringe l'equazione $g(x,y)=0$ al rettangolo $W$ l'insieme delle sue soluzioni è il grafico di $f$. Inoltre
    $$
        \frac{\dd f}{\dd x}(x)=-\frac{\partial_x g(x,f(x))}{\partial_y g(x,f(x))}
    $$
\end{theorem}

\begin{proof}
    [Dimostrazione per $n=2$ e $k=1$.]
    La dimostrazione si articola in tre passaggi: innanzitutto si costruisce $f$, poi ne si verifica la continuità e infine si calcola la sua derivata.


    Per ipotesi, $\partial_yg(x_0,y_0) \neq 0$. Ai fini della dimostrazione, si supponga che $\partial_yg(x_0,y_0) > 0$. Per il teorema di permanenza del segno (Thm. \ref{thm:sign}, Cap. \ref{chap:functions}), esiste una scatola $W=[x_0-\delta,x_0+\delta]\times[y_0-\varepsilon,y_0+\varepsilon]\with \delta,\varepsilon>0 \tc \partial_yg(x,y)>0\ \forall (x,y)\in W$. Sia $h(y)=g(x_0,y)\in\C{1}$. Per quanto appena detto, $h(y)$ è una funzione strettamente crescente nell'intervallo $[y_0-\varepsilon, y_0+\varepsilon]$, per cui $h(y_0-\varepsilon)<0=h(y_0)<h(y_0+\varepsilon)$. Si è appena dimostrato che nel punto medio delle basi del rettangolo la funzione $g$ assume segni discordi. Applicando nuovamente il teorema di permanenza del segno, questa volta alla funzione $g(x,y)$ sulle basi del rettangolo, si ha che esiste un intervallo $[x_0-\delta ', x_0+\delta ']$ all'interno del quale $g(x,y_0-\varepsilon)<0<g(x,y_0+\varepsilon)$. Si noti che $\delta$ potrebbe non coincidere con $\delta '$, quindi, detto $\delta=\min\{\delta,\delta '\}$, per comodità la base del rettangolo sarà ancora $[x_0-\delta,x_0+\delta]$.
    

    Sia ora la funzione $\tilde{h}(y)=g(\overline{x},y) \in \C{1} \with \overline{x} \in [x_0-\delta,x_0+\delta]$ definita per $y \in [y_0-\varepsilon,y_0+\varepsilon]$. Per quanto detto in precedenza, $\tilde{h}(y_0-\varepsilon)<0<\tilde{h}(y_0+\varepsilon)$. Di conseguenza, per il teorema degli zeri, $\exists!\ \overline{y} \in [y_0-\varepsilon,y_0+\varepsilon] \tc \tilde{h}(\overline{y})=g(\overline{x},\overline{y})=0$. È quindi possibile definire $f:[x_0-\delta,x_0+\delta]\to[y_0-\varepsilon,y_0+\varepsilon] \tc f(\overline{x})=\overline{y} \ \forall\, \overline{x} \in [x_0-\delta,x_0+\delta]$.


    Sia $(\overline{x},f(\overline{x}))\in W$. Per verificare la continuità di $f$, si fissi $\overline{\varepsilon}>0$. Allora, per lo stesso ragionamento applicato in precedenza, $g(\overline{x},f(\overline{x})-\overline{\varepsilon})<0=g(\overline{x},f(\overline{x}))<g(\overline{x}, f(\overline{x})+\overline{\varepsilon})$. Per questo motivo, $\exists \overline{\delta} \tc g(x,f(\overline{x})-\overline{\varepsilon}) < 0 < g(x,f(\overline{x})+\overline{\varepsilon}) \ \forall x \in [\overline{x}-\overline{\delta},\overline{x}+\overline{\delta}]\cap[x_0-\delta,x_0+\delta]$. Questo garantisce che $f(\overline{x})-\overline{\varepsilon} < f(x) < f(\overline{x})+\overline{\varepsilon}$, che è esattamente la definizione di continuità (Def. \ref{def:f_cont}, Cap. \ref{chap:functions}).


    Per calcolare la derivata di $f$ in $x_1$, che è $\displaystyle\lim_{x_2\to x_1}\frac{f(x_2)-f(x_1)}{x_2-x_1}$, si introduce la funzione ausiliaria $\bm \varphi=(\varphi_1,\varphi_2):[0,1]\to W$ definita come segue:
    $$
        \begin{cases}
            x=\varphi_1(t)=x_1+t(x_2-x_1)\\
            y=\varphi_2(t)=f(x_1)+t(f(x_2)-f(x_1))
        \end{cases}
    $$
    Sia ora $h(t)=(g\circ\bm\varphi)(t) \in \C{1}([0,1],\R)$. Per il teorema del valor medio di Lagrange, $h(1)-h(0)=h'(c) \with c \in [0,1]$.
    \begin{equation}
        h(1)-h(0)=g(\bm\varphi(1))-g(\bm\varphi(0))=g(x_2,f(x_2))-g(x_1,f(x_1))=0\label{eq:h1_h0}
    \end{equation}
    Inoltre,
    \begin{align}
        h'(c)
        &=\ip{\grad g(\bm\varphi(c))}{\bm\varphi'(c)}=\notag\\
        &=\frac{\partial g}{\partial x}(\bm\varphi(c))[x_2-x_1]+\frac{\partial g}{\partial y}(\bm\varphi(c))[f(x_2)-f(x_1)]\label{eq:der_h}
    \end{align}
    Siano ora $\xi=\xi(c)=x_1+c(x_2-x_1) \e \eta=\eta(c)=f(x_1)+c(f(x_2)-f(x_1))$. Unendo i risultati delle equazioni \eqref{eq:h1_h0} e \eqref{eq:der_h} e considerando che $\partial_yg(x,y)\neq 0 \ \forall (x,y) \in W$, si conclude che
    \begin{gather*}
        \frac{\partial g}{\partial x}(\xi,\eta)[x_2-x_1]+\frac{\partial g}{\partial y}(\xi,\eta)[f(x_2)-f(x_1)]=0\\
        \frac{f(x_2)-f(x_1)}{x_2-x_1}=-\frac{\partial_x g(\xi,\eta)}{\partial_y g(\xi,\eta)}
    \end{gather*}
    Se $x_2$ tende a $x_1$, $f(x_2)\to f(x_1)$ e $\eta \to f(x_1)$ perché $f$ è continua e, essendo $\xi$ intermedio fra $x_2$ e $x_1$, $\xi \to x_1$. In conclusione,
    \begin{align*}
        \frac{\dd f}{\dd x}(x_1)&=\lim_{x_2\to x_1}-\frac{\partial_x g(\xi,\eta)}{\partial_y g(\xi,\eta)}=\\
        &=\lim_{(\xi,\eta)\to(x_1,f(x_1))}-\frac{\partial_x g(\xi,\eta)}{\partial_y g(\xi,\eta)}=\\
        &=-\frac{\partial_x g(x_1,f(x_1))}{\partial_y g(x_1,f(x_1))}
    \end{align*}
\end{proof}

\begin{remark}
    Questa è alta pasticceria.
\end{remark}

\begin{theorem}
    [Caso generale con $k$ equazioni in $n$ incognite]
    Siano $A\subseteq \R^n$ aperto, $\bm g \in \C{1}(A,\R^k)$, $(\vb{x^0}, \vb{y^0})=(x_1^0,\dots,x_{n-k}^0,y_1^0,\dots,y_{k}^0) \in A \tc \bm g(\vb{x^0}, \vb{y^0})=\vb{0} \e$
    $$
        \det\frac{\partial(g_1\cdots g_k)}{\partial (y_1 \cdots y_k)}(\vb{x^0}, \vb{y^0})=\\
        \det\begin{bmatrix}
            \frac{\partial g_1}{\partial y_1} & \cdots & \frac{\partial g_1}{\partial y_k}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial g_k}{\partial y_1} & \cdots & \frac{\partial g_k}{\partial y_k}
        \end{bmatrix}
        (\vb{x^0}, \vb{y^0}) \neq 0
    $$
    Allora esistono $I_{\vb{x^0}} \in \R^{n-k}$ intorno circolare aperto di $\vb{x^0}$ e $J_{\vb{y^0}} \in \R^k$ intorno circolare aperto di $\vb{y^0}$ tali che:
    \begin{enumerate}
        \item $W=I_{\vb{x^0}}\times J_{\vb{y^0}} \subseteq A$
        \item $\exists \vecf \in \C{1}(I_{\vb{x^0}}, J_{\vb{y^0}}) \tc \bm g(\vb{x}, \vb{y})=\vb{0} \iff \vb{y} = \vecf (\vb{x}) \with \vb{x} \in I_{\vb{x^0}}$
    \end{enumerate}
    Inoltre
    $$
        \frac{\partial f_i}{\partial x_j}(\vb{x})=-\frac
        {\det \frac{\partial (g_1,\dots,g_k)}{\partial (y_1,\dots,y_{i-1},x_j,y_{i+1},\dots,y_k)}(\vb{x},\vecf(\vb{x}))}
        {\det \frac{\partial (g_1,\dots,g_k)}{\partial (y_1,\dots,y_k)}(\vb{x},\vecf(\vb{x}))}
    $$
    \qed
\end{theorem}

\section{Estremanti condizionati}

\begin{definition}
    [Punto estremante condizionato]
    Siano $A\subseteq\R^n$ aperto, $\Gamma \subseteq A$ una varietà regolare $(n-k)$-dimensionale e $f: A\to\R$. Il punto $\vb{x_0} \in \Gamma$ è un punto di minimo (massimo) locale per $f$ ristretta a $\Gamma$ se $\exists \varepsilon > 0 \tc f(\vb{x}) \geq f(\vb{x_0}) \ (f(\vb{x}) \leq f(\vb{x_0})) \ \forall \vb{x} \in \Gamma \cap B_\varepsilon(\vb{x_0})$.
\end{definition}

\begin{definition}
    [Punto critico condizionato]
    Siano $A\subseteq\R^n$ aperto, $\Gamma \subseteq A$ una varietà regolare $(n-k)$-dimensionale e $f: A\to\R$. Il punto $\vb{x_0} \in \Gamma$ è punto critico condizionato per $f$ ristretta a $\Gamma$ se $\forall \bm{\hat{\nu}} \in T_{\vb{x_0}}\Gamma, \ \displaystyle\frac{\partial f}{\partial \bm{\hat{\nu}}}(\vb{x_0})=0$. 
\end{definition}

\begin{theorem}
    [di Fermat condizionato]\label{thm:fermat_cond}
    Siano $A\subseteq\R^n$ aperto, $\Gamma \subseteq A$ una varietà regolare $(n-k)$-dimensionale e $f\in\C{1}(A,\R)$. Se $\vb{x_0} \in \Gamma$ è punto estremante condizionato per $f$ a $\Gamma$, allora $\vb{x_0}$ è un punto critico condizionato per $f$ a $\Gamma$.
\end{theorem}

\begin{proof}
    Sia $\bm\varphi\in \C{1}((-\delta,\delta),\Gamma) \tc \bm\varphi(0)=\vb{x_0} \e \bm\varphi'(0)=\hat{\bm\nu}\in T_{\vb{x_0}}\Gamma \tc \norm{\hat{\bm\nu}}=1$.
    Sia $F=(f\circ\bm\varphi):(-\delta,\delta)\to\R$. Per costruzione, $t=0$ è un punto di minimo o di massimo locale per $F$.
    Di conseguenza, per il teorema di Fermat in una dimensione,
    \begin{align*}
        0&=F'(0)=\frac{\dd }{\dd t}(f\circ\bm\varphi)(0)=\\
        &=\ip{\grad f(\bm\varphi(0))}{\bm\varphi'(0)}=\\
        &=\ip{\grad f(\vb{x_0})}{\hat{\bm\nu}}=\frac{\partial f}{\partial \hat{\bm\nu}}(\vb{x_0})
    \end{align*}
    Si ha così la tesi.
\end{proof}

\begin{corollary}\label{cor:fermat_cond}
    Sotto le ipotesi del teorema \ref{thm:fermat_cond}, $\grad f (\vb{x_0}) \in N_{\vb{x_0}}\Gamma$. Questo fatto ne dà un'intuitiva interpretazione geometrica.
    \qed
\end{corollary}

\begin{definition}
    [Funzione di Lagrange]
    Per lo studio dei punti critici condizionati di $f\in\C{1}(A,\R)$ alla varietà regolare $\Gamma$ definita dall'equazione $\bm g (\vb{x})=\vb{0}$ si definisce la funzione di Lagrange, o lagrangiana:
    $$
        \displaystyle \mathcal{L}(\vb{x};\lambda_1,\dots,\lambda_k)=f(\vb{x})-\sum_{j=1}^k \lambda_j g_j(\vb{x}) : A \times \R^k \to \R
    $$
\end{definition}

\begin{theorem}
    [dei moltiplicatori di Lagrange]
    Siano $f,g_1,\dots,g_k \in \C{1}(A\subseteq\R^n, \R) \with A$ aperto in $\R^n$. Se $\Gamma = \{ \vb{x} \in A: g_1 (\vb{x})=0, \dots, g_k(\vb{x})=0 \}$ è una varietà regolare e $\vb{x_0} \in \Gamma$ è punto estremante condizionato di $f$ a $\Gamma$, allora $\exists (\overline{\lambda}_1, \dots,\overline{\lambda}_k)$ tale che il punto $(\vb{x_0}; \overline{\lambda}_1,\dots,\overline{\lambda}_k)$ è punto critico libero di $\mathcal{L}$.
\end{theorem}

\begin{proof}
    Il gradiente di $\mathcal{L}$ è
    $$
        \grad \mathcal{L}(\vb{x};\lambda_1,\dots,\lambda_k)=\left(\grad f(\vb{x})-\sum_{j=1}^{k}\lambda_j\grad g_j(\vb{x}),-g_1(\vb{x}),\dots,-g_k(\vb{x})\right)
    $$
    Se $\vb{x_0}$ è un punto estremante condizionato per $f$ a $\Gamma$, allora $\grad f(\vb{x_0})\in N_{\vb{x_0}}\Gamma$ per il corollario \ref{cor:fermat_cond}. Questo significa che $\exists \overline{\lambda}_1,\dots,\overline{\lambda}_k \tc \grad f(\vb{x_0}) = \sum\limits_{j=1}^k\overline{\lambda}_j\grad g_j(\vb{x_0})$. Di conseguenza, le prime $n$ componenti del gradiente di $\mathcal{L}$ in $\vb{x_0}$ sono nulle. Le ultime $k$ componenti sono nulle in $\vb{x_0}$ perché $\vb{x_0}\in\Gamma$ quindi è soluzione del sistema di equazioni $\vb{g}(\vb{x})=\vb{0}$.
    In conclusione, $(\vb{x_0},\overline{\lambda}_1,\dots,\overline{\lambda}_k)$ è un punto critico libero per $\mathcal{L}$.
\end{proof}

\begin{theorem}
    [Condizioni con funzione di Lagrange]
    Siano $f,g_1,\dots,g_k \in \C{2}(A\subseteq\R^n, \R) \with A$ aperto e $\Gamma = \{ \vb{x} \in A: g_1 (\vb{x})=0, \dots, g_k(\vb{x})=0 \}$ una varietà regolare.
    \begin{enumerate}
        \item Se $\vb{x_0} \in \Gamma$ è punto di minimo (massimo) condizionato di $f$ a $\Gamma$, allora $\exists \overline{\lambda}_1,\dots,\overline{\lambda}_k \in \R \tc$
        \begin{enumerate}[a.]
            \item $\grad f(\vb{x_0})=\sum\limits_{j=1}^k \overline{\lambda}_j \grad g_j(\vb{x_0})$
            \item $\ip{\vb{h}}{\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)\vb{h}} \geq 0 \ (\leq 0) \ \forall \vb{h} \in T_{\vb{x_0}}\Gamma$, ovvero la restrizione della forma quadratica associata alla matrice $\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)$ allo spazio $T_{\vb{x_0}}\Gamma$ è definita positiva o semidefinita positiva (definita negativa o semidefinita negativa)
        \end{enumerate}
        \item Se $\vb{x_0} \in \Gamma$ soddisfa per una qualche scelta di $(\overline{\lambda}_1,\dots,\overline{\lambda}_k)$:
        \begin{enumerate}[a.]
            \item $\grad f(\vb{x_0})=\sum\limits_{j=1}^k\overline{\lambda}_j \grad g_j (\vb{x_0})$
            \item $\ip{\vb{h}}{\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)\vb{h}} > 0\ (<0) \ \forall \vb{h} \in T_{\vb{x_0}}\Gamma$
        \end{enumerate}
        allora $\vb{x_0}$ è punto di minimo (massimo) condizionato di $f$ a $\Gamma$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    [Dimostrazione del punto 2.]
    La matrice hessiana della funzione langrangiana è la seguente:
    $$
        H_{\mathcal{L}}(\vb{x})=
        \begin{bmatrix}
            H_f(\vb{x})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x})
            & -[J_{\vb{g}}(\vb{x})]^T\\
            -J_{\vb{g}}(\vb{x}) & \vb{O}
        \end{bmatrix}
    $$
    Si supponga che essa sia definita positiva se ristretta allo spazio tangente a $\Gamma$ in $\vb{x_0}$.
    Applicando la formula di Taylor,
    \begin{align*}
        f(\vb{x})-f(\vb{x_0})&=\mathcal{L}(\vb{x};\overline{\lambda}_1,\dots,\overline{\lambda}_k)-\mathcal{L}(\vb{x_0};\overline{\lambda}_1,\dots,\overline{\lambda}_k)=\\
        &=\frac{1}{2}\ip{(\vb{x}-\vb{x_0},\vb{0})}{H_{\mathcal{L}}(\vb{x_0})(\vb{x}-\vb{x_0},\vb{0})}+o(\norm{\vb{x}-\vb{x_0}}^2)=\\
        &=\frac{1}{2}\ip{\vb{x}-\vb{x_0}}{\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)(\vb{x}-\vb{x_0})}+o(\norm{\vb{x}-\vb{x_0}}^2)
    \end{align*}
    Sia ora $\vb{x}=\bm\varphi(t)\in \C{1}((-\delta,\delta),\Gamma) \tc \bm\varphi(0)=\vb{x_0} \e \bm\varphi'(0)=\vb{h} \in T_{\vb{x_0}}\Gamma$. Allora $\vb{x}=\vb{x_0}+t\vb{h}+\vb{o}(\abs{t})$ per $\abs{t}\to 0$. Di conseguenza,
    \begin{gather*}
        f(\vb{x})-f(\vb{x_0})=\frac{t^2}{2}\ip{\vb{h}+\vb{o}(1)}{\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)(\vb{h}+\vb{o}(1))}+o(\norm{\vb{h}+\vb{o}(1)}^2t^2)=\\
        =\frac{t^2}{2}\ip{\vb{h}}{\left( H_f(\vb{x_0})-\sum_{j=1}^k\overline{\lambda}_j H_{g_j}(\vb{x_0}) \right)\vb{h}}(1+o(1))\geq 0
    \end{gather*}
    Quindi $\vb{x_0}$ è un punto di minimo per $f$ condizionato a $\Gamma$.
\end{proof}